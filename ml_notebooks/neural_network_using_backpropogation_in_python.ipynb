{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNRbMJbq13JGedhb5qXKok7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["Based on Ajay Srinivas' YouTube video.$^*$\n","\n","\\* [youtu.be/7qYtIveJ6hU](https://youtu.be/7qYtIveJ6hU)"],"metadata":{"id":"YV0cKtdUY-Wr"}},{"cell_type":"code","execution_count":92,"metadata":{"id":"GVLzlQckU3vA","executionInfo":{"status":"ok","timestamp":1667018477207,"user_tz":-480,"elapsed":3,"user":{"displayName":"Sascha Robitzki","userId":"17419209573331928568"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["X = hours sleeping, hours studying <br/>\n","Y = test score of the student"],"metadata":{"id":"eg32zdJ5VMoU"}},{"cell_type":"code","source":["X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n","y = np.array(([92], [86], [89]), dtype=float)\n","\n","print(f'X.shape = {X.shape}')\n","print(X)\n","print(f'y.shape = {y.shape}')\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0rRrsnvGVGSE","executionInfo":{"status":"ok","timestamp":1667018478391,"user_tz":-480,"elapsed":15,"user":{"displayName":"Sascha Robitzki","userId":"17419209573331928568"}},"outputId":"4cf6979b-b53a-4156-8404-2346b5289160"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["X.shape = (3, 2)\n","[[2. 9.]\n"," [1. 5.]\n"," [3. 6.]]\n","y.shape = (3, 1)\n","[[92.]\n"," [86.]\n"," [89.]]\n"]}]},{"cell_type":"code","source":["# Scale units\n","\n","X = X / np.amax(X, axis=0) # maximum of X array\n","print(X)\n","\n","y /= 100 # maximum test score is 100\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rGhI9KajWmpv","executionInfo":{"status":"ok","timestamp":1667018478392,"user_tz":-480,"elapsed":13,"user":{"displayName":"Sascha Robitzki","userId":"17419209573331928568"}},"outputId":"1cd8aed1-c90f-45d8-cef9-21a87e1773d9"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","[[0.92]\n"," [0.86]\n"," [0.89]]\n"]}]},{"cell_type":"code","source":["def sigmoid(x, diff:bool=False) -> np.ndarray:\n","    if diff == True:\n","        return x * (1 - x)\n","    return 1 / (1 + np.exp(- x))\n","\n","class NeuralNetwork(object):\n","    def __init__(self, x:np.array, y:np.array, n_input:int=2, n_hidden:int=3, n_output:int=1, lr:float=0.001):\n","        self.x = x\n","        self.y = y\n","\n","        # parameters\n","        self.input_size = n_input\n","        self.hidden_size = n_hidden\n","        self.output_size = n_output\n","\n","        self.lr = lr\n","\n","        # weights\n","        self.W1 = np.random.randn(self.hidden_size, self.input_size) - 0.5 # 3 x 2 weight matrix from input to hidden layer\n","        self.W2 = np.random.rand(self.output_size, self.hidden_size) - 0.5 # 1 x 3 weight matrix from hidden to output layer\n","\n","        # biases\n","        self.b1 = np.random.randn(self.hidden_size, 1) - 0.5\n","        self.b2 = np.random.rand(self.output_size, 1) - 0.5\n","\n","    def forward_prop(self):\n","        # forward propagation through the network\n","\n","        self.z1 = np.dot(self.x, self.W1.T) + self.b1 # dot product of x (input) and first set of weights (2x3)\n","        self.a1 = sigmoid(self.z1) # activation function\n","        self.z2 = np.dot(self.a1, self.W2.T) + self.b2 # dot product of hidden layer (z1) and second set of weights (3x1)\n","        self.a2 = sigmoid(self.z2)\n","\n","    def backward_prop(self):\n","        # backward propagate through the network\n","\n","        delta_z2 = self.a2 - self.y\n","        self.delta_a2 = delta_z2 * sigmoid(self.a2, diff=True)\n","        self.delta_b2 = 1. / self.hidden_size * np.sum(delta_z2)\n","\n","        self.delta_a1 = (self.delta_a2.dot(self.W2)) * sigmoid(self.a1, diff=True)\n","        self.delta_b1 = 1. / self.hidden_size * np.sum(self.delta_a1)\n","\n","    def optimize(self):\n","        self.W1 += self.x.T.dot(self.lr * self.delta_a1).T # adjusting first set (input -> hidden) weights\n","        self.b1 = self.b1 - self.lr * self.delta_b1\n","        self.W2 += self.a1.T.dot(self.lr * self.delta_a2).T # adjusting second set (hidden -> output) weights\n","        self.b2 = self.b2 - self.lr * self.delta_b2\n","\n","    def train(self):\n","        self.forward_prop()\n","        self.backward_prop()\n","        self.optimize()\n","\n","\n","#np.random.seed(123456)\n","\n","n_epochs = 1000\n","learning_rate = 0.1\n","\n","nn = NeuralNetwork(X,\n","                   y,\n","                   n_input=X.shape[1],\n","                   n_hidden=3,\n","                   lr=learning_rate)\n","\n","for n in range(n_epochs):\n","    nn.train()\n","\n","    if n % 100 == 0:\n","        print(f'Loss: {np.mean(np.square(y - nn.a2))}')\n","\n","print(f'Input = {X}\\n')\n","print(f'Labels = {y}\\n')\n","print(f'Loss = {np.mean(np.square(y - nn.a2))}\\n')\n","print(f'Predicted output = {nn.a2}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChK5-sgnXX70","executionInfo":{"status":"ok","timestamp":1667020102575,"user_tz":-480,"elapsed":328,"user":{"displayName":"Sascha Robitzki","userId":"17419209573331928568"}},"outputId":"2269507e-c9e1-4232-f5de-cd1d9b61bd99"},"execution_count":160,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: 0.25159806149057123\n","Loss: 0.006614708148956777\n","Loss: 0.0006560523669503601\n","Loss: 0.00017218969535482545\n","Loss: 0.0001214909116924936\n","Loss: 0.00012032149733058919\n","Loss: 0.00012469267515110804\n","Loss: 0.00012935234029556961\n","Loss: 0.0001339179566244715\n","Loss: 0.00013850011454911106\n","Input = [[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","\n","Labels = [[0.92]\n"," [0.86]\n"," [0.89]]\n","\n","Loss = 0.00014314019038247\n","\n","Predicted output = [[0.90437349]\n"," [0.87307034]\n"," [0.89379462]]\n"]}]}]}